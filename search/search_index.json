{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>hadal <code>/\u02c8he\u026ad\u0259l/</code> is a simple and efficient tool for mining and aligning sentences with pre-trained models.</p>"},{"location":"#implemented-methods","title":"Implemented methods","text":"Level Method Alignment type sentence margin-based <code>one-to-one</code>"},{"location":"installation/","title":"Installation","text":"<p>NB!</p> <ul> <li>The tool relies on faiss for fast similarity search. To install faiss, please follow the official installation guide.</li> <li>pytorch is also required.</li> </ul> Latest (w/o faiss and pytorch)DevelopmentNo dependencies <pre><code>pip install hadal\n</code></pre> <pre><code>pip install hadal[dev]\npip install hadal[docs]\n</code></pre> <pre><code>pip install hadal --no-deps\n</code></pre>"},{"location":"parallel_text/","title":"What is Parallel Sentence Mining?","text":"<p>Parallel sentence mining is a process of searching parallel (translated) sentence pairs in monolingual corpora.</p> source (English)<pre><code>This wine bar - restaurant promises you beautiful culinary surprises.\nEvery cup of coffee should create a personal moment of pleasure.\nSome text that is not translated.\n</code></pre> target (French)<pre><code>Chaque tasse de caf\u00e9 devrait cr\u00e9er un moment de plaisir personnel.\nDeux\nCe bar \u00e0 vins - restaurant vous promet de belles surprises culinaires.\n</code></pre> <p>The goal is to identify all translation pairs between the <code>source</code> and <code>target</code> sets of sentences.</p> source target index This wine bar - restaurant promises you beautiful culinary surprises. Ce bar \u00e0 vins - restaurant vous promet de belles surprises culinaires. 1 - 3 Every cup of coffee should create a personal moment of pleasure. Chaque tasse de caf\u00e9 devrait cr\u00e9er un moment de plaisir personnel. 2 - 1"},{"location":"api_reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>custom_logger</li> <li>faiss_search</li> <li>huggingface_automodel</li> <li>parallel_sentence_mining<ul> <li>margin_based<ul> <li>margin_based</li> <li>margin_based_tools</li> </ul> </li> </ul> </li> </ul>"},{"location":"api_reference/custom_logger/","title":"custom_logger","text":""},{"location":"api_reference/custom_logger/#custom_logger","title":"custom_logger","text":"<p>This module contains the <code>fuction default custom logger</code> that can be used to create a logger with a specified name, level, and format.</p>"},{"location":"api_reference/custom_logger/#custom_logger.default_custom_logger","title":"default_custom_logger","text":"<pre><code>default_custom_logger(\n    name: str, level: int | None = logging.DEBUG, log_format: str | None = None\n) -&gt; logging.Logger\n</code></pre> <p>Create a logger with a specified name, level, and format.</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>The name of the logger.</p> </li> <li> <code>level</code>             (<code>int | None</code>, default:                 <code>logging.DEBUG</code> )         \u2013          <p>Logging level.</p> </li> <li> <code>log_format</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The format of the log messages. If <code>None</code>, the default format is <code>%(asctime)s | %(name)s | %(module)s | %(levelname)s | %(message)s</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>logger</code> (            <code>logging.Logger</code> )        \u2013          <p>A logger object with the specified name, logging level, and format.</p> </li> </ul> Source code in <code>hadal/custom_logger.py</code> <pre><code>def default_custom_logger(\n    name: str,\n    level: int | None = logging.DEBUG,\n    log_format: str | None = None,\n) -&gt; logging.Logger:\n    \"\"\"Create a logger with a specified name, level, and format.\n\n    Args:\n        name (str): The name of the logger.\n        level (int | None, optional): Logging level.\n        log_format (str | None, optional): The format of the log messages. If `None`, the default format is `%(asctime)s | %(name)s | %(module)s | %(levelname)s | %(message)s`.\n\n    Returns:\n        logger (logging.Logger): A logger object with the specified name, logging level, and format.\n    \"\"\"\n    if log_format is None:\n        log_format = \"%(asctime)s | %(name)s | %(module)s | %(levelname)s | %(message)s\"\n\n    datefmt = \"%Y-%m-%d %H:%M:%S\"\n\n    logger = logging.basicConfig(level=level, format=log_format, datefmt=datefmt, encoding=\"utf-8\")\n    logger = logging.getLogger(name=name)\n\n    return logger\n</code></pre>"},{"location":"api_reference/faiss_search/","title":"faiss_search","text":""},{"location":"api_reference/faiss_search/#faiss_search","title":"faiss_search","text":"<p>This module contains the <code>class FaissSearch</code> that can be used to perform k-nearest neighbor search using the Faiss library.</p>"},{"location":"api_reference/faiss_search/#faiss_search.FaissSearch","title":"FaissSearch","text":"<p>Class to perform k-nearest neighbor search using the Faiss library.</p> <p>Methods:</p> <ul> <li> <code>k_nearest_neighbors</code>           \u2013            <p>Perform k-nearest neighbor search using Faiss.</p> </li> </ul> Source code in <code>hadal/faiss_search.py</code> <pre><code>class FaissSearch:\n    \"\"\"Class to perform k-nearest neighbor search using the Faiss library.\n\n    Methods:\n        k_nearest_neighbors: Perform k-nearest neighbor search using Faiss.\n    \"\"\"\n\n    def __init__(self, device: str | None = None, *, enable_logging: bool = True, log_level: int | None = logging.INFO) -&gt; None:\n        \"\"\"Initialize FaissSearch object.\n\n        Args:\n            device (str | None, optional): Device for the Faiss search. If `None`, it will use GPU if available, otherwise CPU. Default is `None`.\n            enable_logging (bool, optional): Logging option.\n            log_level (int | None, optional): Logging level.\n        \"\"\"\n        if enable_logging is True:\n            self.logger = default_custom_logger(name=__name__, level=log_level)\n        else:\n            self.logger = logging.getLogger(__name__)\n            self.logger.disabled = True\n\n        if device is None:\n            device = \"cuda\" if faiss.get_num_gpus() &gt; 0 else \"cpu\"\n            self.logger.info(\"Faiss device: %s\", device)\n\n        self._target_device = device\n\n    def k_nearest_neighbors(\n        self,\n        source_embeddings: numpy.ndarray,\n        target_embeddings: numpy.ndarray,\n        k: int = 4,\n        knn_metric: str = \"inner_product\",\n        device: str | None = None,\n    ) -&gt; tuple[numpy.ndarray, numpy.ndarray]:\n        \"\"\"Perform k-nearest neighbor search using Faiss.\n\n        Args:\n            source_embeddings (numpy.ndarray): The source embeddings.\n            target_embeddings (numpy.ndarray): The target embeddings.\n            k (int, optional): The number of nearest neighbors.\n            knn_metric (str, optional): The metric to use for k-nearest neighbor search. Can be `inner_product` or `sqeuclidean`.\n            device (str | None, optional): The device to use for Faiss search. If `None`, it will use GPU if available, otherwise CPU.\n\n        Note:\n            It is fully relying on the Faiss library for the k-nearest neighbor search `faiss.knn` and `faiss.gpu_knn`.\n\n            - `inner_product` uses `faiss.METRIC_INNER_PRODUCT`\n            - `sqeuclidean` uses `faiss.METRIC_L2` (squared Euclidean distance)\n\n        Returns:\n            - d (numpy.ndarray): The distances of the k-nearest neighbors.\n            - ind (numpy.ndarray): The indices of the k-nearest neighbors.\n        \"\"\"\n        if device is None:\n            device = self._target_device\n\n        self.logger.info(\"Perform k-nearest neighbor search...\")\n\n        if knn_metric == \"inner_product\":\n            knn_metric = faiss.METRIC_INNER_PRODUCT\n        elif knn_metric == \"sqeuclidean\":\n            # squared Euclidean (L2) distance\n            knn_metric = faiss.METRIC_L2\n\n        if device == \"cpu\":\n            self.logger.info(\"Using faiss knn on CPU...\")\n            # https://github.com/facebookresearch/faiss/blob/d85601d972af2d64103769ab8d940db28aaae2a0/faiss/python/extra_wrappers.py#L330\n            d, ind = faiss.knn(xq=source_embeddings, xb=target_embeddings, k=k, metric=knn_metric)\n        else:\n            self.logger.info(\"Using faiss knn on GPU...\")\n            res = faiss.StandardGpuResources()\n            d, ind = faiss.gpu_knn(res=res, xq=source_embeddings, xb=target_embeddings, k=k, metric_type=knn_metric)\n\n        self.logger.info(\"Done k-nearest neighbor search!\")\n        return d, ind\n</code></pre>"},{"location":"api_reference/faiss_search/#faiss_search.FaissSearch.__init__","title":"__init__","text":"<pre><code>__init__(\n    device: str | None = None,\n    *,\n    enable_logging: bool = True,\n    log_level: int | None = logging.INFO\n) -&gt; None\n</code></pre> <p>Initialize FaissSearch object.</p> <p>Parameters:</p> <ul> <li> <code>device</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Device for the Faiss search. If <code>None</code>, it will use GPU if available, otherwise CPU. Default is <code>None</code>.</p> </li> <li> <code>enable_logging</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Logging option.</p> </li> <li> <code>log_level</code>             (<code>int | None</code>, default:                 <code>logging.INFO</code> )         \u2013          <p>Logging level.</p> </li> </ul> Source code in <code>hadal/faiss_search.py</code> <pre><code>def __init__(self, device: str | None = None, *, enable_logging: bool = True, log_level: int | None = logging.INFO) -&gt; None:\n    \"\"\"Initialize FaissSearch object.\n\n    Args:\n        device (str | None, optional): Device for the Faiss search. If `None`, it will use GPU if available, otherwise CPU. Default is `None`.\n        enable_logging (bool, optional): Logging option.\n        log_level (int | None, optional): Logging level.\n    \"\"\"\n    if enable_logging is True:\n        self.logger = default_custom_logger(name=__name__, level=log_level)\n    else:\n        self.logger = logging.getLogger(__name__)\n        self.logger.disabled = True\n\n    if device is None:\n        device = \"cuda\" if faiss.get_num_gpus() &gt; 0 else \"cpu\"\n        self.logger.info(\"Faiss device: %s\", device)\n\n    self._target_device = device\n</code></pre>"},{"location":"api_reference/faiss_search/#faiss_search.FaissSearch.k_nearest_neighbors","title":"k_nearest_neighbors","text":"<pre><code>k_nearest_neighbors(\n    source_embeddings: numpy.ndarray,\n    target_embeddings: numpy.ndarray,\n    k: int = 4,\n    knn_metric: str = \"inner_product\",\n    device: str | None = None,\n) -&gt; tuple[numpy.ndarray, numpy.ndarray]\n</code></pre> <p>Perform k-nearest neighbor search using Faiss.</p> <p>Parameters:</p> <ul> <li> <code>source_embeddings</code>             (<code>numpy.ndarray</code>)         \u2013          <p>The source embeddings.</p> </li> <li> <code>target_embeddings</code>             (<code>numpy.ndarray</code>)         \u2013          <p>The target embeddings.</p> </li> <li> <code>k</code>             (<code>int</code>, default:                 <code>4</code> )         \u2013          <p>The number of nearest neighbors.</p> </li> <li> <code>knn_metric</code>             (<code>str</code>, default:                 <code>'inner_product'</code> )         \u2013          <p>The metric to use for k-nearest neighbor search. Can be <code>inner_product</code> or <code>sqeuclidean</code>.</p> </li> <li> <code>device</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The device to use for Faiss search. If <code>None</code>, it will use GPU if available, otherwise CPU.</p> </li> </ul> Note <p>It is fully relying on the Faiss library for the k-nearest neighbor search <code>faiss.knn</code> and <code>faiss.gpu_knn</code>.</p> <ul> <li><code>inner_product</code> uses <code>faiss.METRIC_INNER_PRODUCT</code></li> <li><code>sqeuclidean</code> uses <code>faiss.METRIC_L2</code> (squared Euclidean distance)</li> </ul> <p>Returns:</p> <ul> <li> <code>numpy.ndarray</code>         \u2013          <ul> <li>d (numpy.ndarray): The distances of the k-nearest neighbors.</li> </ul> </li> <li> <code>numpy.ndarray</code>         \u2013          <ul> <li>ind (numpy.ndarray): The indices of the k-nearest neighbors.</li> </ul> </li> </ul> Source code in <code>hadal/faiss_search.py</code> <pre><code>def k_nearest_neighbors(\n    self,\n    source_embeddings: numpy.ndarray,\n    target_embeddings: numpy.ndarray,\n    k: int = 4,\n    knn_metric: str = \"inner_product\",\n    device: str | None = None,\n) -&gt; tuple[numpy.ndarray, numpy.ndarray]:\n    \"\"\"Perform k-nearest neighbor search using Faiss.\n\n    Args:\n        source_embeddings (numpy.ndarray): The source embeddings.\n        target_embeddings (numpy.ndarray): The target embeddings.\n        k (int, optional): The number of nearest neighbors.\n        knn_metric (str, optional): The metric to use for k-nearest neighbor search. Can be `inner_product` or `sqeuclidean`.\n        device (str | None, optional): The device to use for Faiss search. If `None`, it will use GPU if available, otherwise CPU.\n\n    Note:\n        It is fully relying on the Faiss library for the k-nearest neighbor search `faiss.knn` and `faiss.gpu_knn`.\n\n        - `inner_product` uses `faiss.METRIC_INNER_PRODUCT`\n        - `sqeuclidean` uses `faiss.METRIC_L2` (squared Euclidean distance)\n\n    Returns:\n        - d (numpy.ndarray): The distances of the k-nearest neighbors.\n        - ind (numpy.ndarray): The indices of the k-nearest neighbors.\n    \"\"\"\n    if device is None:\n        device = self._target_device\n\n    self.logger.info(\"Perform k-nearest neighbor search...\")\n\n    if knn_metric == \"inner_product\":\n        knn_metric = faiss.METRIC_INNER_PRODUCT\n    elif knn_metric == \"sqeuclidean\":\n        # squared Euclidean (L2) distance\n        knn_metric = faiss.METRIC_L2\n\n    if device == \"cpu\":\n        self.logger.info(\"Using faiss knn on CPU...\")\n        # https://github.com/facebookresearch/faiss/blob/d85601d972af2d64103769ab8d940db28aaae2a0/faiss/python/extra_wrappers.py#L330\n        d, ind = faiss.knn(xq=source_embeddings, xb=target_embeddings, k=k, metric=knn_metric)\n    else:\n        self.logger.info(\"Using faiss knn on GPU...\")\n        res = faiss.StandardGpuResources()\n        d, ind = faiss.gpu_knn(res=res, xq=source_embeddings, xb=target_embeddings, k=k, metric_type=knn_metric)\n\n    self.logger.info(\"Done k-nearest neighbor search!\")\n    return d, ind\n</code></pre>"},{"location":"api_reference/huggingface_automodel/","title":"huggingface_automodel","text":""},{"location":"api_reference/huggingface_automodel/#huggingface_automodel","title":"huggingface_automodel","text":"<p>This module contains the <code>class HuggingfaceAutoModel</code> that can be used to encode text using a Huggingface AutoModel.</p>"},{"location":"api_reference/huggingface_automodel/#huggingface_automodel.HuggingfaceAutoModel","title":"HuggingfaceAutoModel","text":"<p>Class to encode text using a Huggingface AutoModel.</p> <p>Methods:</p> <ul> <li> <code>encode</code>           \u2013            <p>Encode text using a Huggingface AutoModel.</p> </li> </ul> Source code in <code>hadal/huggingface_automodel.py</code> <pre><code>class HuggingfaceAutoModel:\n    \"\"\"Class to encode text using a Huggingface AutoModel.\n\n    Methods:\n        encode: Encode text using a Huggingface AutoModel.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str | pathlib.Path,\n        device: str | None = None,\n        *,\n        enable_logging: bool = True,\n        log_level: int | None = logging.INFO,\n    ) -&gt; None:\n        \"\"\"Initialize HuggingfaceAutoModel object.\n\n        Args:\n            model_name_or_path (str | pathlib.Path): Name or path to the pre-trained model.\n            device (str | None, optional): Device for the model.\n            enable_logging (bool, optional): Logging option.\n            log_level (int | None, optional): Logging level.\n        \"\"\"\n        if enable_logging is True:\n            self.logger = default_custom_logger(name=__name__, level=log_level)\n        else:\n            self.logger = logging.getLogger(__name__)\n            self.logger.disabled = True\n\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            self.logger.info(\"Pytorch device: %s\", device)\n\n        self._target_device = torch.device(device)\n\n        self.model = AutoModel.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n        self.model.eval()\n        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n        self.config = AutoConfig.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n\n        if model_name_or_path is not None and model_name_or_path != \"\":\n            self.logger.info(\"Load huggingface model: %s\", model_name_or_path)\n\n    def encode(\n        self,\n        sentences: str | list[str],\n        batch_size: int = 32,\n        output_value: str = \"pooler_output\",\n        convert_to: str | None = None,\n        *,\n        normalize_embeddings: bool = False,\n        device: str | None = None,\n    ) -&gt; list[torch.Tensor] | torch.Tensor | numpy.ndarray:\n        \"\"\"Encode text using a Huggingface AutoModel.\n\n        Args:\n            sentences (str | list[str]): The sentences to encode.\n            batch_size (int, optional): The batch size.\n            output_value (str, optional): Model output type. Can be `pooler_output` or `last_hidden_state`.\n            convert_to (str | None, optional): Convert the embeddings to `torch` or `numpy` format. If `torch`, it will return a `torch.Tensor`. If `numpy`, it will return a `numpy.ndarray`. If `None`, it will return a `list[torch.Tensor]`.\n            normalize_embeddings (bool, optional): Normalize the embeddings.\n            device (str | None, optional): Device for the model.\n\n        Raises:\n            NotImplementedError: If the `output_value` is not implemented.\n\n        Returns:\n            all_embeddings (list[torch.Tensor] | torch.Tensor | numpy.ndarray): The embeddings of the sentences.\n        \"\"\"\n        if device is None:\n            device = self._target_device\n\n        self.model.to(device)\n        self.logger.info(\"Encoding on pytorch device: %s\", device)\n\n        if isinstance(sentences, str):\n            sentences = [sentences]\n\n        all_embeddings = []\n        length_sorted_idx = numpy.argsort([-self._text_length(sen) for sen in sentences])\n        sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n\n        for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\"):\n            sentences_batch = sentences_sorted[start_index : start_index + batch_size]\n            inputs = self.tokenizer(sentences_batch, return_tensors=\"pt\", truncation=True, padding=True)\n            inputs = batch_to_device(batch=inputs, target_device=device)\n\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n\n                if output_value == \"pooler_output\":\n                    embeddings = outputs.pooler_output\n                elif output_value == \"last_hidden_state\":\n                    embeddings = outputs.last_hidden_state[:, 0, :]\n                else:\n                    msg = f\"output_value=`{output_value}` not implemented\"\n                    raise NotImplementedError(msg)\n\n                if normalize_embeddings is True:\n                    # apply L2 normalization to the embeddings\n                    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n\n                all_embeddings.extend(embeddings)\n\n        all_embeddings: list[torch.Tensor] = [all_embeddings[idx] for idx in numpy.argsort(length_sorted_idx)]\n\n        if convert_to == \"torch\":\n            all_embeddings: torch.Tensor = torch.stack(all_embeddings)\n        elif convert_to == \"numpy\":\n            all_embeddings: numpy.ndarray = torch.stack(all_embeddings).numpy()\n\n        return all_embeddings\n\n    def _text_length(self, text: list[str] | list | str) -&gt; int:\n        \"\"\"Calculate the length of the given sentences.\n\n        Args:\n            text (list[str] | list | str): The sentences.\n\n        Raises:\n            TypeError: Input cannot be a `dict`.\n            TypeError: Input cannot be a `tuple`.\n\n        Returns:\n            length (int): The length of the text.\n        \"\"\"\n        if isinstance(text, dict):\n            msg = \"Input cannot be a `dict`.\"\n            raise TypeError(msg)\n        if isinstance(text, tuple):\n            msg = \"Input cannot be a `tuple`.\"\n            raise TypeError(msg)\n\n        if not hasattr(text, \"__len__\"):  # no len() method\n            return 1\n        if len(text) == 0:  # empty string or list\n            return len(text)\n        return sum([len(t) for t in text])  # sum of length of individual strings\n</code></pre>"},{"location":"api_reference/huggingface_automodel/#huggingface_automodel.HuggingfaceAutoModel.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_name_or_path: str | pathlib.Path,\n    device: str | None = None,\n    *,\n    enable_logging: bool = True,\n    log_level: int | None = logging.INFO\n) -&gt; None\n</code></pre> <p>Initialize HuggingfaceAutoModel object.</p> <p>Parameters:</p> <ul> <li> <code>model_name_or_path</code>             (<code>str | pathlib.Path</code>)         \u2013          <p>Name or path to the pre-trained model.</p> </li> <li> <code>device</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Device for the model.</p> </li> <li> <code>enable_logging</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Logging option.</p> </li> <li> <code>log_level</code>             (<code>int | None</code>, default:                 <code>logging.INFO</code> )         \u2013          <p>Logging level.</p> </li> </ul> Source code in <code>hadal/huggingface_automodel.py</code> <pre><code>def __init__(\n    self,\n    model_name_or_path: str | pathlib.Path,\n    device: str | None = None,\n    *,\n    enable_logging: bool = True,\n    log_level: int | None = logging.INFO,\n) -&gt; None:\n    \"\"\"Initialize HuggingfaceAutoModel object.\n\n    Args:\n        model_name_or_path (str | pathlib.Path): Name or path to the pre-trained model.\n        device (str | None, optional): Device for the model.\n        enable_logging (bool, optional): Logging option.\n        log_level (int | None, optional): Logging level.\n    \"\"\"\n    if enable_logging is True:\n        self.logger = default_custom_logger(name=__name__, level=log_level)\n    else:\n        self.logger = logging.getLogger(__name__)\n        self.logger.disabled = True\n\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.logger.info(\"Pytorch device: %s\", device)\n\n    self._target_device = torch.device(device)\n\n    self.model = AutoModel.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n    self.model.eval()\n    self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n    self.config = AutoConfig.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n\n    if model_name_or_path is not None and model_name_or_path != \"\":\n        self.logger.info(\"Load huggingface model: %s\", model_name_or_path)\n</code></pre>"},{"location":"api_reference/huggingface_automodel/#huggingface_automodel.HuggingfaceAutoModel.encode","title":"encode","text":"<pre><code>encode(\n    sentences: str | list[str],\n    batch_size: int = 32,\n    output_value: str = \"pooler_output\",\n    convert_to: str | None = None,\n    *,\n    normalize_embeddings: bool = False,\n    device: str | None = None\n) -&gt; list[torch.Tensor] | torch.Tensor | numpy.ndarray\n</code></pre> <p>Encode text using a Huggingface AutoModel.</p> <p>Parameters:</p> <ul> <li> <code>sentences</code>             (<code>str | list[str]</code>)         \u2013          <p>The sentences to encode.</p> </li> <li> <code>batch_size</code>             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>The batch size.</p> </li> <li> <code>output_value</code>             (<code>str</code>, default:                 <code>'pooler_output'</code> )         \u2013          <p>Model output type. Can be <code>pooler_output</code> or <code>last_hidden_state</code>.</p> </li> <li> <code>convert_to</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Convert the embeddings to <code>torch</code> or <code>numpy</code> format. If <code>torch</code>, it will return a <code>torch.Tensor</code>. If <code>numpy</code>, it will return a <code>numpy.ndarray</code>. If <code>None</code>, it will return a <code>list[torch.Tensor]</code>.</p> </li> <li> <code>normalize_embeddings</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Normalize the embeddings.</p> </li> <li> <code>device</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Device for the model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>           \u2013          <p>If the <code>output_value</code> is not implemented.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>all_embeddings</code> (            <code>list[torch.Tensor] | torch.Tensor | numpy.ndarray</code> )        \u2013          <p>The embeddings of the sentences.</p> </li> </ul> Source code in <code>hadal/huggingface_automodel.py</code> <pre><code>def encode(\n    self,\n    sentences: str | list[str],\n    batch_size: int = 32,\n    output_value: str = \"pooler_output\",\n    convert_to: str | None = None,\n    *,\n    normalize_embeddings: bool = False,\n    device: str | None = None,\n) -&gt; list[torch.Tensor] | torch.Tensor | numpy.ndarray:\n    \"\"\"Encode text using a Huggingface AutoModel.\n\n    Args:\n        sentences (str | list[str]): The sentences to encode.\n        batch_size (int, optional): The batch size.\n        output_value (str, optional): Model output type. Can be `pooler_output` or `last_hidden_state`.\n        convert_to (str | None, optional): Convert the embeddings to `torch` or `numpy` format. If `torch`, it will return a `torch.Tensor`. If `numpy`, it will return a `numpy.ndarray`. If `None`, it will return a `list[torch.Tensor]`.\n        normalize_embeddings (bool, optional): Normalize the embeddings.\n        device (str | None, optional): Device for the model.\n\n    Raises:\n        NotImplementedError: If the `output_value` is not implemented.\n\n    Returns:\n        all_embeddings (list[torch.Tensor] | torch.Tensor | numpy.ndarray): The embeddings of the sentences.\n    \"\"\"\n    if device is None:\n        device = self._target_device\n\n    self.model.to(device)\n    self.logger.info(\"Encoding on pytorch device: %s\", device)\n\n    if isinstance(sentences, str):\n        sentences = [sentences]\n\n    all_embeddings = []\n    length_sorted_idx = numpy.argsort([-self._text_length(sen) for sen in sentences])\n    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n\n    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\"):\n        sentences_batch = sentences_sorted[start_index : start_index + batch_size]\n        inputs = self.tokenizer(sentences_batch, return_tensors=\"pt\", truncation=True, padding=True)\n        inputs = batch_to_device(batch=inputs, target_device=device)\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n            if output_value == \"pooler_output\":\n                embeddings = outputs.pooler_output\n            elif output_value == \"last_hidden_state\":\n                embeddings = outputs.last_hidden_state[:, 0, :]\n            else:\n                msg = f\"output_value=`{output_value}` not implemented\"\n                raise NotImplementedError(msg)\n\n            if normalize_embeddings is True:\n                # apply L2 normalization to the embeddings\n                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n\n            all_embeddings.extend(embeddings)\n\n    all_embeddings: list[torch.Tensor] = [all_embeddings[idx] for idx in numpy.argsort(length_sorted_idx)]\n\n    if convert_to == \"torch\":\n        all_embeddings: torch.Tensor = torch.stack(all_embeddings)\n    elif convert_to == \"numpy\":\n        all_embeddings: numpy.ndarray = torch.stack(all_embeddings).numpy()\n\n    return all_embeddings\n</code></pre>"},{"location":"api_reference/huggingface_automodel/#huggingface_automodel.HuggingfaceAutoModel._text_length","title":"_text_length","text":"<pre><code>_text_length(text: list[str] | list | str) -&gt; int\n</code></pre> <p>Calculate the length of the given sentences.</p> <p>Parameters:</p> <ul> <li> <code>text</code>             (<code>list[str] | list | str</code>)         \u2013          <p>The sentences.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>           \u2013          <p>Input cannot be a <code>dict</code>.</p> </li> <li> <code>TypeError</code>           \u2013          <p>Input cannot be a <code>tuple</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>length</code> (            <code>int</code> )        \u2013          <p>The length of the text.</p> </li> </ul> Source code in <code>hadal/huggingface_automodel.py</code> <pre><code>def _text_length(self, text: list[str] | list | str) -&gt; int:\n    \"\"\"Calculate the length of the given sentences.\n\n    Args:\n        text (list[str] | list | str): The sentences.\n\n    Raises:\n        TypeError: Input cannot be a `dict`.\n        TypeError: Input cannot be a `tuple`.\n\n    Returns:\n        length (int): The length of the text.\n    \"\"\"\n    if isinstance(text, dict):\n        msg = \"Input cannot be a `dict`.\"\n        raise TypeError(msg)\n    if isinstance(text, tuple):\n        msg = \"Input cannot be a `tuple`.\"\n        raise TypeError(msg)\n\n    if not hasattr(text, \"__len__\"):  # no len() method\n        return 1\n    if len(text) == 0:  # empty string or list\n        return len(text)\n    return sum([len(t) for t in text])  # sum of length of individual strings\n</code></pre>"},{"location":"api_reference/huggingface_automodel/#huggingface_automodel.batch_to_device","title":"batch_to_device","text":"<pre><code>batch_to_device(batch, target_device: torch.device)\n</code></pre> <p>Move a batch of tensors to the specified device.</p> Source code in <code>hadal/huggingface_automodel.py</code> <pre><code>def batch_to_device(batch, target_device: torch.device):  # noqa: ANN201, ANN001\n    \"\"\"Move a batch of tensors to the specified device.\"\"\"\n    for key in batch:\n        if isinstance(batch[key], torch.Tensor):\n            batch[key] = batch[key].to(target_device)\n    return batch\n</code></pre>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based/","title":"margin_based","text":""},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based/#parallel_sentence_mining.margin_based.margin_based","title":"parallel_sentence_mining.margin_based.margin_based","text":"<p>This module contains the <code>class MarginBasedPipeline</code> that implements the margin-based pipeline.</p>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based/#parallel_sentence_mining.margin_based.margin_based.MarginBasedPipeline","title":"MarginBasedPipeline","text":"<p>Class that implements the margin-based pipeline.</p> <p>Methods:</p> <ul> <li> <code>make_alignments</code>           \u2013            <p>Make sentence alignments.</p> </li> </ul> Source code in <code>hadal/parallel_sentence_mining/margin_based/margin_based.py</code> <pre><code>class MarginBasedPipeline:\n    \"\"\"Class that implements the margin-based pipeline.\n\n    Methods:\n        make_alignments: Make sentence alignments.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str | pathlib.Path,\n        model_device: str | None = None,\n        faiss_device: str | None = None,\n        *,\n        enable_logging: bool = True,\n        log_level: int | None = logging.INFO,\n    ) -&gt; None:\n        \"\"\"Initialize a MarginBasedPipeline object.\n\n        Args:\n            model_name_or_path (str | pathlib.Path): Name or path to the pre-trained model.\n            model_device (str | None, optional): Device for the model.\n            faiss_device (str | None, optional): Device for the Faiss search. If `None`, it will use GPU if available, otherwise CPU.\n            enable_logging (bool, optional): Logging option.\n            log_level (int | None, optional):  Logging level.\n        \"\"\"\n        self.model = HuggingfaceAutoModel(\n            model_name_or_path=model_name_or_path,\n            device=model_device,\n            enable_logging=enable_logging,\n        )\n        self.align_method = MarginBased()\n        self.faiss_search = FaissSearch(device=faiss_device, enable_logging=enable_logging)\n        self.model_device = model_device\n        self.faiss_device = faiss_device\n\n        if enable_logging is True:\n            self.logger = default_custom_logger(name=__name__, level=log_level)\n        else:\n            self.logger = logging.getLogger(__name__)\n            self.logger.disabled = True\n\n    def make_alignments(\n        self,\n        source_sentences: list[str],\n        target_sentences: list[str],\n        batch_size: int = 32,\n        output_value: str = \"pooler_output\",\n        convert_to: str = \"numpy\",\n        *,\n        normalize_embeddings: bool = True,\n        knn_neighbors: int = 4,\n        knn_metric: str = \"inner_product\",\n        margin: str = \"ratio\",\n        strategy: str = \"max_score\",\n    ) -&gt; list[tuple[numpy.float64, str, str]]:\n        \"\"\"Make sentence alignments.\n\n        Args:\n            source_sentences (list[str]): Source sentences.\n            target_sentences (list[str]): Target sentences.\n            batch_size (int, optional): The batch size.\n            output_value (str, optional): Model output type. Can be `pooler_output` or `last_hidden_state`.\n            convert_to (str, optional): Convert the embeddings to `torch` or `numpy` format. If `torch`, it will return a `torch.Tensor`. If `numpy`, it will return a `numpy.ndarray`. If `None`, it will return a `list[torch.Tensor]`.\n            normalize_embeddings (bool, optional): Normalize the embeddings.\n            knn_neighbors (int, optional): The number of nearest neighbors.\n            knn_metric (str, optional): The metric to use for k-nearest neighbor search. Can be `inner_product` or `l2`.\n            margin (str, optional): The margin function to use. Valid options are `ratio` and `distance`.\n            strategy (str, optional): The strategy to use for selecting the best candidates.\n\n        Returns:\n            bitext_list (list[tuple[numpy.float64, str, str]]): The `list[tuple[score, source_sentence, target_sentence]]` of the best sentence alignments.\n        \"\"\"\n        self.logger.info(\"Encoding embeddings for source sentences...\")\n        source_embeddings = self.model.encode(\n            sentences=source_sentences,\n            batch_size=batch_size,\n            output_value=output_value,\n            convert_to=convert_to,\n            normalize_embeddings=normalize_embeddings,\n        )\n        self.logger.info(\"Encoding embeddings for target sentences...\")\n        target_embeddings = self.model.encode(\n            sentences=target_sentences,\n            batch_size=batch_size,\n            output_value=output_value,\n            convert_to=convert_to,\n            normalize_embeddings=normalize_embeddings,\n        )\n\n        self.logger.info(\"Perform kNN in both directions...\")\n        self.logger.info(\"Perform kNN in source -&gt; target direction\")\n        x2y_sim, x2y_ind = self.faiss_search.k_nearest_neighbors(\n            source_embeddings,\n            target_embeddings,\n            k=knn_neighbors,\n            knn_metric=knn_metric,\n        )\n        x2y_mean = x2y_sim.mean(axis=1)\n\n        self.logger.info(\"Perform kNN in target -&gt; source direction\")\n        y2x_sim, y2x_ind = self.faiss_search.k_nearest_neighbors(\n            target_embeddings,\n            source_embeddings,\n            k=knn_neighbors,\n            knn_metric=knn_metric,\n        )\n        y2x_mean = y2x_sim.mean(axis=1)\n\n        self.logger.info(\"%s margin is selected\", margin)\n        chosen_margin = self.align_method.select_margin(margin=margin)\n\n        self.logger.info(\"Compute forward and backward scores...\")\n        fwd_scores = self.align_method.margin_based_score_candidates(\n            source_embeddings,\n            target_embeddings,\n            x2y_ind,\n            x2y_mean,\n            y2x_mean,\n            margin=chosen_margin,\n        )\n        bwd_scores = self.align_method.margin_based_score_candidates(\n            target_embeddings,\n            source_embeddings,\n            y2x_ind,\n            y2x_mean,\n            x2y_mean,\n            margin=chosen_margin,\n        )\n\n        self.logger.info(\"Selecting best candidates...\")\n        indices, scores = self.align_method.select_best_candidates(\n            source_embeddings,\n            x2y_ind,\n            fwd_scores,\n            target_embeddings,\n            y2x_ind,\n            bwd_scores,\n            strategy=strategy,\n        )\n\n        bitext_list = self.align_method.get_sentence_pairs(indices, scores, source_sentences, target_sentences)\n\n        self.logger.info(\"Output sentences: pairs %d\", len(bitext_list))\n\n        return bitext_list\n</code></pre>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based/#parallel_sentence_mining.margin_based.margin_based.MarginBasedPipeline.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_name_or_path: str | pathlib.Path,\n    model_device: str | None = None,\n    faiss_device: str | None = None,\n    *,\n    enable_logging: bool = True,\n    log_level: int | None = logging.INFO\n) -&gt; None\n</code></pre> <p>Initialize a MarginBasedPipeline object.</p> <p>Parameters:</p> <ul> <li> <code>model_name_or_path</code>             (<code>str | pathlib.Path</code>)         \u2013          <p>Name or path to the pre-trained model.</p> </li> <li> <code>model_device</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Device for the model.</p> </li> <li> <code>faiss_device</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Device for the Faiss search. If <code>None</code>, it will use GPU if available, otherwise CPU.</p> </li> <li> <code>enable_logging</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Logging option.</p> </li> <li> <code>log_level</code>             (<code>int | None</code>, default:                 <code>logging.INFO</code> )         \u2013          <p>Logging level.</p> </li> </ul> Source code in <code>hadal/parallel_sentence_mining/margin_based/margin_based.py</code> <pre><code>def __init__(\n    self,\n    model_name_or_path: str | pathlib.Path,\n    model_device: str | None = None,\n    faiss_device: str | None = None,\n    *,\n    enable_logging: bool = True,\n    log_level: int | None = logging.INFO,\n) -&gt; None:\n    \"\"\"Initialize a MarginBasedPipeline object.\n\n    Args:\n        model_name_or_path (str | pathlib.Path): Name or path to the pre-trained model.\n        model_device (str | None, optional): Device for the model.\n        faiss_device (str | None, optional): Device for the Faiss search. If `None`, it will use GPU if available, otherwise CPU.\n        enable_logging (bool, optional): Logging option.\n        log_level (int | None, optional):  Logging level.\n    \"\"\"\n    self.model = HuggingfaceAutoModel(\n        model_name_or_path=model_name_or_path,\n        device=model_device,\n        enable_logging=enable_logging,\n    )\n    self.align_method = MarginBased()\n    self.faiss_search = FaissSearch(device=faiss_device, enable_logging=enable_logging)\n    self.model_device = model_device\n    self.faiss_device = faiss_device\n\n    if enable_logging is True:\n        self.logger = default_custom_logger(name=__name__, level=log_level)\n    else:\n        self.logger = logging.getLogger(__name__)\n        self.logger.disabled = True\n</code></pre>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based/#parallel_sentence_mining.margin_based.margin_based.MarginBasedPipeline.make_alignments","title":"make_alignments","text":"<pre><code>make_alignments(\n    source_sentences: list[str],\n    target_sentences: list[str],\n    batch_size: int = 32,\n    output_value: str = \"pooler_output\",\n    convert_to: str = \"numpy\",\n    *,\n    normalize_embeddings: bool = True,\n    knn_neighbors: int = 4,\n    knn_metric: str = \"inner_product\",\n    margin: str = \"ratio\",\n    strategy: str = \"max_score\"\n) -&gt; list[tuple[numpy.float64, str, str]]\n</code></pre> <p>Make sentence alignments.</p> <p>Parameters:</p> <ul> <li> <code>source_sentences</code>             (<code>list[str]</code>)         \u2013          <p>Source sentences.</p> </li> <li> <code>target_sentences</code>             (<code>list[str]</code>)         \u2013          <p>Target sentences.</p> </li> <li> <code>batch_size</code>             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>The batch size.</p> </li> <li> <code>output_value</code>             (<code>str</code>, default:                 <code>'pooler_output'</code> )         \u2013          <p>Model output type. Can be <code>pooler_output</code> or <code>last_hidden_state</code>.</p> </li> <li> <code>convert_to</code>             (<code>str</code>, default:                 <code>'numpy'</code> )         \u2013          <p>Convert the embeddings to <code>torch</code> or <code>numpy</code> format. If <code>torch</code>, it will return a <code>torch.Tensor</code>. If <code>numpy</code>, it will return a <code>numpy.ndarray</code>. If <code>None</code>, it will return a <code>list[torch.Tensor]</code>.</p> </li> <li> <code>normalize_embeddings</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Normalize the embeddings.</p> </li> <li> <code>knn_neighbors</code>             (<code>int</code>, default:                 <code>4</code> )         \u2013          <p>The number of nearest neighbors.</p> </li> <li> <code>knn_metric</code>             (<code>str</code>, default:                 <code>'inner_product'</code> )         \u2013          <p>The metric to use for k-nearest neighbor search. Can be <code>inner_product</code> or <code>l2</code>.</p> </li> <li> <code>margin</code>             (<code>str</code>, default:                 <code>'ratio'</code> )         \u2013          <p>The margin function to use. Valid options are <code>ratio</code> and <code>distance</code>.</p> </li> <li> <code>strategy</code>             (<code>str</code>, default:                 <code>'max_score'</code> )         \u2013          <p>The strategy to use for selecting the best candidates.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bitext_list</code> (            <code>list[tuple[numpy.float64, str, str]]</code> )        \u2013          <p>The <code>list[tuple[score, source_sentence, target_sentence]]</code> of the best sentence alignments.</p> </li> </ul> Source code in <code>hadal/parallel_sentence_mining/margin_based/margin_based.py</code> <pre><code>def make_alignments(\n    self,\n    source_sentences: list[str],\n    target_sentences: list[str],\n    batch_size: int = 32,\n    output_value: str = \"pooler_output\",\n    convert_to: str = \"numpy\",\n    *,\n    normalize_embeddings: bool = True,\n    knn_neighbors: int = 4,\n    knn_metric: str = \"inner_product\",\n    margin: str = \"ratio\",\n    strategy: str = \"max_score\",\n) -&gt; list[tuple[numpy.float64, str, str]]:\n    \"\"\"Make sentence alignments.\n\n    Args:\n        source_sentences (list[str]): Source sentences.\n        target_sentences (list[str]): Target sentences.\n        batch_size (int, optional): The batch size.\n        output_value (str, optional): Model output type. Can be `pooler_output` or `last_hidden_state`.\n        convert_to (str, optional): Convert the embeddings to `torch` or `numpy` format. If `torch`, it will return a `torch.Tensor`. If `numpy`, it will return a `numpy.ndarray`. If `None`, it will return a `list[torch.Tensor]`.\n        normalize_embeddings (bool, optional): Normalize the embeddings.\n        knn_neighbors (int, optional): The number of nearest neighbors.\n        knn_metric (str, optional): The metric to use for k-nearest neighbor search. Can be `inner_product` or `l2`.\n        margin (str, optional): The margin function to use. Valid options are `ratio` and `distance`.\n        strategy (str, optional): The strategy to use for selecting the best candidates.\n\n    Returns:\n        bitext_list (list[tuple[numpy.float64, str, str]]): The `list[tuple[score, source_sentence, target_sentence]]` of the best sentence alignments.\n    \"\"\"\n    self.logger.info(\"Encoding embeddings for source sentences...\")\n    source_embeddings = self.model.encode(\n        sentences=source_sentences,\n        batch_size=batch_size,\n        output_value=output_value,\n        convert_to=convert_to,\n        normalize_embeddings=normalize_embeddings,\n    )\n    self.logger.info(\"Encoding embeddings for target sentences...\")\n    target_embeddings = self.model.encode(\n        sentences=target_sentences,\n        batch_size=batch_size,\n        output_value=output_value,\n        convert_to=convert_to,\n        normalize_embeddings=normalize_embeddings,\n    )\n\n    self.logger.info(\"Perform kNN in both directions...\")\n    self.logger.info(\"Perform kNN in source -&gt; target direction\")\n    x2y_sim, x2y_ind = self.faiss_search.k_nearest_neighbors(\n        source_embeddings,\n        target_embeddings,\n        k=knn_neighbors,\n        knn_metric=knn_metric,\n    )\n    x2y_mean = x2y_sim.mean(axis=1)\n\n    self.logger.info(\"Perform kNN in target -&gt; source direction\")\n    y2x_sim, y2x_ind = self.faiss_search.k_nearest_neighbors(\n        target_embeddings,\n        source_embeddings,\n        k=knn_neighbors,\n        knn_metric=knn_metric,\n    )\n    y2x_mean = y2x_sim.mean(axis=1)\n\n    self.logger.info(\"%s margin is selected\", margin)\n    chosen_margin = self.align_method.select_margin(margin=margin)\n\n    self.logger.info(\"Compute forward and backward scores...\")\n    fwd_scores = self.align_method.margin_based_score_candidates(\n        source_embeddings,\n        target_embeddings,\n        x2y_ind,\n        x2y_mean,\n        y2x_mean,\n        margin=chosen_margin,\n    )\n    bwd_scores = self.align_method.margin_based_score_candidates(\n        target_embeddings,\n        source_embeddings,\n        y2x_ind,\n        y2x_mean,\n        x2y_mean,\n        margin=chosen_margin,\n    )\n\n    self.logger.info(\"Selecting best candidates...\")\n    indices, scores = self.align_method.select_best_candidates(\n        source_embeddings,\n        x2y_ind,\n        fwd_scores,\n        target_embeddings,\n        y2x_ind,\n        bwd_scores,\n        strategy=strategy,\n    )\n\n    bitext_list = self.align_method.get_sentence_pairs(indices, scores, source_sentences, target_sentences)\n\n    self.logger.info(\"Output sentences: pairs %d\", len(bitext_list))\n\n    return bitext_list\n</code></pre>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based_tools/","title":"margin_based_tools","text":""},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based_tools/#parallel_sentence_mining.margin_based.margin_based_tools","title":"parallel_sentence_mining.margin_based.margin_based_tools","text":"<p>The module contains the <code>class MarginBased</code> that implements the margin-based scoring for parallel sentence mining.</p>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based_tools/#parallel_sentence_mining.margin_based.margin_based_tools.MarginBased","title":"MarginBased","text":"<p>Class that implements the margin-based scoring for parallel sentence mining.</p> <p>Methods:</p> <ul> <li> <code>select_margin</code>           \u2013            <p>Select the margin function.</p> </li> <li> <code>margin_based_score</code>           \u2013            <p>Compute the margin-based score.</p> </li> <li> <code>margin_based_score_candidates</code>           \u2013            <p>Compute the margin-based scores for a batch of sentence pairs.</p> </li> <li> <code>select_best_candidates</code>           \u2013            <p>Select the best sentence pairs.</p> </li> <li> <code>get_sentence_pairs</code>           \u2013            <p>Get the sentence pairs.</p> </li> </ul> Source code in <code>hadal/parallel_sentence_mining/margin_based/margin_based_tools.py</code> <pre><code>class MarginBased:\n    \"\"\"Class that implements the margin-based scoring for parallel sentence mining.\n\n    Methods:\n        select_margin: Select the margin function.\n        margin_based_score: Compute the margin-based score.\n        margin_based_score_candidates: Compute the margin-based scores for a batch of sentence pairs.\n        select_best_candidates: Select the best sentence pairs.\n        get_sentence_pairs: Get the sentence pairs.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize a MarginBased object.\"\"\"\n\n    def select_margin(self, margin: str = \"ratio\") -&gt; Callable:\n        \"\"\"Select the margin function.\n\n        Source: https://arxiv.org/pdf/1811.01136.pdf 3.1 Margin-based scoring\n\n        Args:\n            margin (str, optional): The margin function to use. Valid options are `ratio` and `distance`.\n\n        Raises:\n            NotImplementedError: If the given `margin` is not implemented.\n\n        Returns:\n            margin_func (Callable): The margin function.\n        \"\"\"\n        if margin == \"ratio\":\n            margin_func = lambda a, b: a / b  # noqa\n        elif margin == \"distance\":\n            margin_func = lambda a, b: a - b  # noqa\n        else:\n            msg = f\"margin=`{margin}` is not implemented\"\n            raise NotImplementedError(msg)\n        return margin_func\n\n    def margin_based_score(\n        self,\n        source_embeddings: numpy.ndarray,\n        target_embeddings: numpy.ndarray,\n        fwd_mean: numpy.ndarray,\n        bwd_mean: numpy.ndarray,\n        margin_func: Callable,\n    ) -&gt; numpy.ndarray:\n        \"\"\"Compute the margin-based score.\n\n        Source: https://arxiv.org/pdf/1811.01136.pdf 3.1 Margin-based scoring\n\n        Args:\n            source_embeddings (numpy.ndarray): Source embeddings.\n            target_embeddings (numpy.ndarray): Target embeddings.\n            fwd_mean (numpy.ndarray): The forward mean.\n            bwd_mean (numpy.ndarray): The backward mean.\n            margin_func (Callable): The margin function.\n\n        Returns:\n            score (numpy.ndarray): Margin-based score.\n        \"\"\"\n        score = margin_func(source_embeddings.dot(target_embeddings), (fwd_mean + bwd_mean) / 2)\n\n        return score\n\n    def margin_based_score_candidates(\n        self,\n        source_embeddings: numpy.ndarray,\n        target_embeddings: numpy.ndarray,\n        candidate_inds: numpy.ndarray,\n        fwd_mean: numpy.ndarray,\n        bwd_mean: numpy.ndarray,\n        margin: Callable,\n    ) -&gt; numpy.ndarray:\n        \"\"\"Compute the margin-based scores for a batch of sentence pairs.\n\n        Args:\n            source_embeddings (numpy.ndarray): Source embeddings.\n            target_embeddings (numpy.ndarray): Target embeddings.\n            candidate_inds (numpy.ndarray): The indices of the candidate target embeddings for each source embedding.\n            fwd_mean (numpy.ndarray): The forward mean.\n            bwd_mean (numpy.ndarray): The backward mean.\n            margin (Callable): The margin function.\n\n        Returns:\n            scores (numpy.ndarray): The margin-based scores for the candidate pairs.\n        \"\"\"\n        scores = numpy.zeros(candidate_inds.shape)\n        for i in range(scores.shape[0]):\n            for j in range(scores.shape[1]):\n                k = candidate_inds[i, j]\n                scores[i, j] = self.margin_based_score(\n                    source_embeddings[i],\n                    target_embeddings[k],\n                    fwd_mean[i],\n                    bwd_mean[k],\n                    margin,\n                )\n        return scores\n\n    def select_best_candidates(\n        self,\n        source_embeddings: numpy.ndarray,\n        x2y_ind: numpy.ndarray,\n        fwd_scores: numpy.ndarray,\n        target_embeddings: numpy.ndarray,\n        y2x_ind: numpy.ndarray,\n        bwd_scores: numpy.ndarray,\n        strategy: str = \"max_score\",\n    ) -&gt; tuple[numpy.ndarray, numpy.ndarray]:\n        \"\"\"Select the best sentence pairs.\n\n        Source: https://arxiv.org/pdf/1811.01136.pdf 3.2 Candidate generation and filtering (only max. score)\n\n        Args:\n            source_embeddings (numpy.ndarray): Source embeddings.\n            x2y_ind (numpy.ndarray): Indices of the target sentences corresponding to each source sentence.\n            fwd_scores (numpy.ndarray): Scores of the forward alignment between source and target sentences.\n            target_embeddings (numpy.ndarray): Target embeddings.\n            y2x_ind (numpy.ndarray): Indices of the source sentences corresponding to each target sentence.\n            bwd_scores (numpy.ndarray): Scores of the backward alignment between target and source sentences.\n            strategy (str, optional): The strategy to use for selecting the best candidates.\n\n        Raises:\n            NotImplementedError: If the given `strategy` is not implemented.\n\n        Returns:\n            - indices (numpy.ndarray): An array of indices representing the sentence pairs.\n            - scores (numpy.ndarray): An array of scores representing the similarity between the sentence pairs.\n        \"\"\"\n        if strategy == \"max_score\":\n            fwd_best = x2y_ind[numpy.arange(source_embeddings.shape[0]), fwd_scores.argmax(axis=1)]\n            bwd_best = y2x_ind[numpy.arange(target_embeddings.shape[0]), bwd_scores.argmax(axis=1)]\n\n            indices = numpy.stack(\n                [\n                    numpy.concatenate([numpy.arange(source_embeddings.shape[0]), bwd_best]),\n                    numpy.concatenate([fwd_best, numpy.arange(target_embeddings.shape[0])]),\n                ],\n                axis=1,\n            )\n            scores = numpy.concatenate([fwd_scores.max(axis=1), bwd_scores.max(axis=1)])\n\n        else:\n            msg = f\"`{strategy}` is not implemented\"\n            raise NotImplementedError(msg)\n\n        return indices, scores\n\n    def get_sentence_pairs(\n        self,\n        indices: numpy.ndarray,\n        scores: numpy.ndarray,\n        source_sentences: list[str],\n        target_sentences: list[str],\n    ) -&gt; list[tuple[numpy.float64, str, str]]:\n        \"\"\"Get the sentence pairs.\n\n        Args:\n            indices (numpy.ndarray): An array of indices representing the sentence pairs.\n            scores (numpy.ndarray): An array of scores representing the similarity between the sentence pairs.\n            source_sentences (list[str]): Source sentences.\n            target_sentences (list[str]): Target sentences.\n\n        Returns:\n            bitext_list (list[tuple[numpy.float64, str, str]]): A list of tuples with score, source sentences and target sentences.\n        \"\"\"\n        seen_src, seen_trg = set(), set()\n\n        bitext_list = []\n\n        for i in numpy.argsort(-scores):\n            src_ind, trg_ind = indices[i]\n            src_ind = int(src_ind)\n            trg_ind = int(trg_ind)\n\n            if src_ind not in seen_src and trg_ind not in seen_trg:\n                seen_src.add(src_ind)\n                seen_trg.add(trg_ind)\n                rounded_score = numpy.round(scores[i], 4)\n                bitext_list.append((rounded_score, source_sentences[src_ind], target_sentences[trg_ind]))\n\n        return bitext_list\n</code></pre>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based_tools/#parallel_sentence_mining.margin_based.margin_based_tools.MarginBased.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize a MarginBased object.</p> Source code in <code>hadal/parallel_sentence_mining/margin_based/margin_based_tools.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize a MarginBased object.\"\"\"\n</code></pre>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based_tools/#parallel_sentence_mining.margin_based.margin_based_tools.MarginBased.select_margin","title":"select_margin","text":"<pre><code>select_margin(margin: str = 'ratio') -&gt; Callable\n</code></pre> <p>Select the margin function.</p> <p>Source: https://arxiv.org/pdf/1811.01136.pdf 3.1 Margin-based scoring</p> <p>Parameters:</p> <ul> <li> <code>margin</code>             (<code>str</code>, default:                 <code>'ratio'</code> )         \u2013          <p>The margin function to use. Valid options are <code>ratio</code> and <code>distance</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>           \u2013          <p>If the given <code>margin</code> is not implemented.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>margin_func</code> (            <code>Callable</code> )        \u2013          <p>The margin function.</p> </li> </ul> Source code in <code>hadal/parallel_sentence_mining/margin_based/margin_based_tools.py</code> <pre><code>def select_margin(self, margin: str = \"ratio\") -&gt; Callable:\n    \"\"\"Select the margin function.\n\n    Source: https://arxiv.org/pdf/1811.01136.pdf 3.1 Margin-based scoring\n\n    Args:\n        margin (str, optional): The margin function to use. Valid options are `ratio` and `distance`.\n\n    Raises:\n        NotImplementedError: If the given `margin` is not implemented.\n\n    Returns:\n        margin_func (Callable): The margin function.\n    \"\"\"\n    if margin == \"ratio\":\n        margin_func = lambda a, b: a / b  # noqa\n    elif margin == \"distance\":\n        margin_func = lambda a, b: a - b  # noqa\n    else:\n        msg = f\"margin=`{margin}` is not implemented\"\n        raise NotImplementedError(msg)\n    return margin_func\n</code></pre>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based_tools/#parallel_sentence_mining.margin_based.margin_based_tools.MarginBased.margin_based_score","title":"margin_based_score","text":"<pre><code>margin_based_score(\n    source_embeddings: numpy.ndarray,\n    target_embeddings: numpy.ndarray,\n    fwd_mean: numpy.ndarray,\n    bwd_mean: numpy.ndarray,\n    margin_func: Callable,\n) -&gt; numpy.ndarray\n</code></pre> <p>Compute the margin-based score.</p> <p>Source: https://arxiv.org/pdf/1811.01136.pdf 3.1 Margin-based scoring</p> <p>Parameters:</p> <ul> <li> <code>source_embeddings</code>             (<code>numpy.ndarray</code>)         \u2013          <p>Source embeddings.</p> </li> <li> <code>target_embeddings</code>             (<code>numpy.ndarray</code>)         \u2013          <p>Target embeddings.</p> </li> <li> <code>fwd_mean</code>             (<code>numpy.ndarray</code>)         \u2013          <p>The forward mean.</p> </li> <li> <code>bwd_mean</code>             (<code>numpy.ndarray</code>)         \u2013          <p>The backward mean.</p> </li> <li> <code>margin_func</code>             (<code>Callable</code>)         \u2013          <p>The margin function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>score</code> (            <code>numpy.ndarray</code> )        \u2013          <p>Margin-based score.</p> </li> </ul> Source code in <code>hadal/parallel_sentence_mining/margin_based/margin_based_tools.py</code> <pre><code>def margin_based_score(\n    self,\n    source_embeddings: numpy.ndarray,\n    target_embeddings: numpy.ndarray,\n    fwd_mean: numpy.ndarray,\n    bwd_mean: numpy.ndarray,\n    margin_func: Callable,\n) -&gt; numpy.ndarray:\n    \"\"\"Compute the margin-based score.\n\n    Source: https://arxiv.org/pdf/1811.01136.pdf 3.1 Margin-based scoring\n\n    Args:\n        source_embeddings (numpy.ndarray): Source embeddings.\n        target_embeddings (numpy.ndarray): Target embeddings.\n        fwd_mean (numpy.ndarray): The forward mean.\n        bwd_mean (numpy.ndarray): The backward mean.\n        margin_func (Callable): The margin function.\n\n    Returns:\n        score (numpy.ndarray): Margin-based score.\n    \"\"\"\n    score = margin_func(source_embeddings.dot(target_embeddings), (fwd_mean + bwd_mean) / 2)\n\n    return score\n</code></pre>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based_tools/#parallel_sentence_mining.margin_based.margin_based_tools.MarginBased.margin_based_score_candidates","title":"margin_based_score_candidates","text":"<pre><code>margin_based_score_candidates(\n    source_embeddings: numpy.ndarray,\n    target_embeddings: numpy.ndarray,\n    candidate_inds: numpy.ndarray,\n    fwd_mean: numpy.ndarray,\n    bwd_mean: numpy.ndarray,\n    margin: Callable,\n) -&gt; numpy.ndarray\n</code></pre> <p>Compute the margin-based scores for a batch of sentence pairs.</p> <p>Parameters:</p> <ul> <li> <code>source_embeddings</code>             (<code>numpy.ndarray</code>)         \u2013          <p>Source embeddings.</p> </li> <li> <code>target_embeddings</code>             (<code>numpy.ndarray</code>)         \u2013          <p>Target embeddings.</p> </li> <li> <code>candidate_inds</code>             (<code>numpy.ndarray</code>)         \u2013          <p>The indices of the candidate target embeddings for each source embedding.</p> </li> <li> <code>fwd_mean</code>             (<code>numpy.ndarray</code>)         \u2013          <p>The forward mean.</p> </li> <li> <code>bwd_mean</code>             (<code>numpy.ndarray</code>)         \u2013          <p>The backward mean.</p> </li> <li> <code>margin</code>             (<code>Callable</code>)         \u2013          <p>The margin function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scores</code> (            <code>numpy.ndarray</code> )        \u2013          <p>The margin-based scores for the candidate pairs.</p> </li> </ul> Source code in <code>hadal/parallel_sentence_mining/margin_based/margin_based_tools.py</code> <pre><code>def margin_based_score_candidates(\n    self,\n    source_embeddings: numpy.ndarray,\n    target_embeddings: numpy.ndarray,\n    candidate_inds: numpy.ndarray,\n    fwd_mean: numpy.ndarray,\n    bwd_mean: numpy.ndarray,\n    margin: Callable,\n) -&gt; numpy.ndarray:\n    \"\"\"Compute the margin-based scores for a batch of sentence pairs.\n\n    Args:\n        source_embeddings (numpy.ndarray): Source embeddings.\n        target_embeddings (numpy.ndarray): Target embeddings.\n        candidate_inds (numpy.ndarray): The indices of the candidate target embeddings for each source embedding.\n        fwd_mean (numpy.ndarray): The forward mean.\n        bwd_mean (numpy.ndarray): The backward mean.\n        margin (Callable): The margin function.\n\n    Returns:\n        scores (numpy.ndarray): The margin-based scores for the candidate pairs.\n    \"\"\"\n    scores = numpy.zeros(candidate_inds.shape)\n    for i in range(scores.shape[0]):\n        for j in range(scores.shape[1]):\n            k = candidate_inds[i, j]\n            scores[i, j] = self.margin_based_score(\n                source_embeddings[i],\n                target_embeddings[k],\n                fwd_mean[i],\n                bwd_mean[k],\n                margin,\n            )\n    return scores\n</code></pre>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based_tools/#parallel_sentence_mining.margin_based.margin_based_tools.MarginBased.select_best_candidates","title":"select_best_candidates","text":"<pre><code>select_best_candidates(\n    source_embeddings: numpy.ndarray,\n    x2y_ind: numpy.ndarray,\n    fwd_scores: numpy.ndarray,\n    target_embeddings: numpy.ndarray,\n    y2x_ind: numpy.ndarray,\n    bwd_scores: numpy.ndarray,\n    strategy: str = \"max_score\",\n) -&gt; tuple[numpy.ndarray, numpy.ndarray]\n</code></pre> <p>Select the best sentence pairs.</p> <p>Source: https://arxiv.org/pdf/1811.01136.pdf 3.2 Candidate generation and filtering (only max. score)</p> <p>Parameters:</p> <ul> <li> <code>source_embeddings</code>             (<code>numpy.ndarray</code>)         \u2013          <p>Source embeddings.</p> </li> <li> <code>x2y_ind</code>             (<code>numpy.ndarray</code>)         \u2013          <p>Indices of the target sentences corresponding to each source sentence.</p> </li> <li> <code>fwd_scores</code>             (<code>numpy.ndarray</code>)         \u2013          <p>Scores of the forward alignment between source and target sentences.</p> </li> <li> <code>target_embeddings</code>             (<code>numpy.ndarray</code>)         \u2013          <p>Target embeddings.</p> </li> <li> <code>y2x_ind</code>             (<code>numpy.ndarray</code>)         \u2013          <p>Indices of the source sentences corresponding to each target sentence.</p> </li> <li> <code>bwd_scores</code>             (<code>numpy.ndarray</code>)         \u2013          <p>Scores of the backward alignment between target and source sentences.</p> </li> <li> <code>strategy</code>             (<code>str</code>, default:                 <code>'max_score'</code> )         \u2013          <p>The strategy to use for selecting the best candidates.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>           \u2013          <p>If the given <code>strategy</code> is not implemented.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>numpy.ndarray</code>         \u2013          <ul> <li>indices (numpy.ndarray): An array of indices representing the sentence pairs.</li> </ul> </li> <li> <code>numpy.ndarray</code>         \u2013          <ul> <li>scores (numpy.ndarray): An array of scores representing the similarity between the sentence pairs.</li> </ul> </li> </ul> Source code in <code>hadal/parallel_sentence_mining/margin_based/margin_based_tools.py</code> <pre><code>def select_best_candidates(\n    self,\n    source_embeddings: numpy.ndarray,\n    x2y_ind: numpy.ndarray,\n    fwd_scores: numpy.ndarray,\n    target_embeddings: numpy.ndarray,\n    y2x_ind: numpy.ndarray,\n    bwd_scores: numpy.ndarray,\n    strategy: str = \"max_score\",\n) -&gt; tuple[numpy.ndarray, numpy.ndarray]:\n    \"\"\"Select the best sentence pairs.\n\n    Source: https://arxiv.org/pdf/1811.01136.pdf 3.2 Candidate generation and filtering (only max. score)\n\n    Args:\n        source_embeddings (numpy.ndarray): Source embeddings.\n        x2y_ind (numpy.ndarray): Indices of the target sentences corresponding to each source sentence.\n        fwd_scores (numpy.ndarray): Scores of the forward alignment between source and target sentences.\n        target_embeddings (numpy.ndarray): Target embeddings.\n        y2x_ind (numpy.ndarray): Indices of the source sentences corresponding to each target sentence.\n        bwd_scores (numpy.ndarray): Scores of the backward alignment between target and source sentences.\n        strategy (str, optional): The strategy to use for selecting the best candidates.\n\n    Raises:\n        NotImplementedError: If the given `strategy` is not implemented.\n\n    Returns:\n        - indices (numpy.ndarray): An array of indices representing the sentence pairs.\n        - scores (numpy.ndarray): An array of scores representing the similarity between the sentence pairs.\n    \"\"\"\n    if strategy == \"max_score\":\n        fwd_best = x2y_ind[numpy.arange(source_embeddings.shape[0]), fwd_scores.argmax(axis=1)]\n        bwd_best = y2x_ind[numpy.arange(target_embeddings.shape[0]), bwd_scores.argmax(axis=1)]\n\n        indices = numpy.stack(\n            [\n                numpy.concatenate([numpy.arange(source_embeddings.shape[0]), bwd_best]),\n                numpy.concatenate([fwd_best, numpy.arange(target_embeddings.shape[0])]),\n            ],\n            axis=1,\n        )\n        scores = numpy.concatenate([fwd_scores.max(axis=1), bwd_scores.max(axis=1)])\n\n    else:\n        msg = f\"`{strategy}` is not implemented\"\n        raise NotImplementedError(msg)\n\n    return indices, scores\n</code></pre>"},{"location":"api_reference/parallel_sentence_mining/margin_based/margin_based_tools/#parallel_sentence_mining.margin_based.margin_based_tools.MarginBased.get_sentence_pairs","title":"get_sentence_pairs","text":"<pre><code>get_sentence_pairs(\n    indices: numpy.ndarray,\n    scores: numpy.ndarray,\n    source_sentences: list[str],\n    target_sentences: list[str],\n) -&gt; list[tuple[numpy.float64, str, str]]\n</code></pre> <p>Get the sentence pairs.</p> <p>Parameters:</p> <ul> <li> <code>indices</code>             (<code>numpy.ndarray</code>)         \u2013          <p>An array of indices representing the sentence pairs.</p> </li> <li> <code>scores</code>             (<code>numpy.ndarray</code>)         \u2013          <p>An array of scores representing the similarity between the sentence pairs.</p> </li> <li> <code>source_sentences</code>             (<code>list[str]</code>)         \u2013          <p>Source sentences.</p> </li> <li> <code>target_sentences</code>             (<code>list[str]</code>)         \u2013          <p>Target sentences.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bitext_list</code> (            <code>list[tuple[numpy.float64, str, str]]</code> )        \u2013          <p>A list of tuples with score, source sentences and target sentences.</p> </li> </ul> Source code in <code>hadal/parallel_sentence_mining/margin_based/margin_based_tools.py</code> <pre><code>def get_sentence_pairs(\n    self,\n    indices: numpy.ndarray,\n    scores: numpy.ndarray,\n    source_sentences: list[str],\n    target_sentences: list[str],\n) -&gt; list[tuple[numpy.float64, str, str]]:\n    \"\"\"Get the sentence pairs.\n\n    Args:\n        indices (numpy.ndarray): An array of indices representing the sentence pairs.\n        scores (numpy.ndarray): An array of scores representing the similarity between the sentence pairs.\n        source_sentences (list[str]): Source sentences.\n        target_sentences (list[str]): Target sentences.\n\n    Returns:\n        bitext_list (list[tuple[numpy.float64, str, str]]): A list of tuples with score, source sentences and target sentences.\n    \"\"\"\n    seen_src, seen_trg = set(), set()\n\n    bitext_list = []\n\n    for i in numpy.argsort(-scores):\n        src_ind, trg_ind = indices[i]\n        src_ind = int(src_ind)\n        trg_ind = int(trg_ind)\n\n        if src_ind not in seen_src and trg_ind not in seen_trg:\n            seen_src.add(src_ind)\n            seen_trg.add(trg_ind)\n            rounded_score = numpy.round(scores[i], 4)\n            bitext_list.append((rounded_score, source_sentences[src_ind], target_sentences[trg_ind]))\n\n    return bitext_list\n</code></pre>"},{"location":"parallel_sentence_mining/margin_based/","title":"Margin-based","text":""},{"location":"parallel_sentence_mining/margin_based/#quickstart","title":"Quickstart","text":"<pre><code>import hadal\n\n# (1) Prepare source and target sentences\n\nsource_test = [\n        \"I think I like wine now.\",\n        \"She eats one apple every day.\",\n        \"They serve pizza dogs in the cafeteria.\",\n        \"Empty sentence.\",\n    ]\n\ntarget_test = [\n        \"Je pense que j'aime le vin maintenant.\",\n        \"Elle mange une pomme chaque jour.\",\n        \"Ils vendent des hot-dogs \u00e0 la caf\u00e9t\u00e9ria.\",\n        \"Ce jeu se joue sur le vaisseau spatial.\",\n        \"Barry est plus tard d\u00e9charg\u00e9 du corps apr\u00e8s sa derni\u00e8re bataille.\",\n    ]\n\n# (2) Load model\n\nmodel_name = \"setu4993/LaBSE\"\n\n# (3) Load alignment config\n\nalignment_config = hadal.MarginBasedPipeline(\n        model_name_or_path=model_name,\n        model_device=\"cpu\",\n        faiss_device=\"cpu\"\n    )\n\n# (4) Make alignments\n\nresult = alignment_config.make_alignments(\n        source_sentences=source_test,\n        target_sentences=target_test,\n        knn_neighbors=2,\n    )\n\nfor ind, (score, src, tgt) in enumerate(result, start=1):\n    print(f\"Pair: {ind}\")\n    print(f\"Score: {score}\\nSource: {src}\\nTarget: {tgt}\\n\")\n</code></pre> <pre><code># (5) Expected output\n\nPair: 1\nScore: 1.5549\nSource: I think I like wine now.\nTarget: Je pense que j'aime le vin maintenant.\n\nPair: 2\nScore: 1.5079\nSource: She eats one apple every day.\nTarget: Elle mange une pomme chaque jour.\n\nPair: 3\nScore: 1.4353\nSource: They serve pizza dogs in the cafeteria.\nTarget: Ils vendent des hot-dogs \u00e0 la caf\u00e9t\u00e9ria.\n\nPair: 4\nScore: 0.4112\nSource: Empty sentence.\nTarget: Ce jeu se joue sur le vaisseau spatial.\n</code></pre>"},{"location":"parallel_sentence_mining/margin_based/#high-level-description","title":"High-level description","text":"<ol> <li>Encode <code>source</code> and <code>target</code> sentences with the model (LaBSE is recommended, or you can try others from huggingface)</li> <li>Find the k nearest neighbor sentences for all sentences in both directions. Choose a value of <code>k</code> between 4 and 16.</li> <li>Score all possible sentence combinations using the formula mentioned in Section 4.3.</li> <li>The pairs with the highest scores are most likely translated sentences. Note that the score can be larger than 1. You may need to set a threshold and ignore pairs below that threshold. A threshold of about 1.2 - 1.3 works well for high-quality results.</li> </ol>"},{"location":"parallel_sentence_mining/margin_based/#acknowledgements","title":"Acknowledgements","text":"<ul> <li>Original paper: Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings</li> <li>Sentence Transformers implementation.</li> </ul>"}]}